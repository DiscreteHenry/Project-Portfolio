{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee9f3b25",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or no access: 'root_dir = \"C:/Users/HenryLi/Downloads/Scans\"/PREFFIR-11115 (Cor CTAAdapt 0.75 B26f 75%).nii.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HenryLi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nibabel\\loadsave.py:101\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     stat_result = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[31mOSError\u001b[39m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'root_dir = \"C:/Users/HenryLi/Downloads/Scans\"/PREFFIR-11115 (Cor CTAAdapt 0.75 B26f 75%).nii.gz'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 190\u001b[39m\n\u001b[32m    188\u001b[39m best_val_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     val_loss, val_preds, val_targets = evaluate(model, val_loader, criterion)\n\u001b[32m    192\u001b[39m     scheduler.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion)\u001b[39m\n\u001b[32m    130\u001b[39m model.train()\n\u001b[32m    131\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HenryLi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HenryLi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HenryLi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mCTADataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     72\u001b[39m entry = \u001b[38;5;28mself\u001b[39m.data.iloc[idx]\n\u001b[32m     73\u001b[39m file_path = os.path.join(\u001b[38;5;28mself\u001b[39m.root_dir, entry[\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m slices = \u001b[43mpreprocess_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (12, 224, 224)\u001b[39;00m\n\u001b[32m     75\u001b[39m slices = torch.tensor(slices).unsqueeze(\u001b[32m1\u001b[39m).float()  \u001b[38;5;66;03m# (12, 1, 224, 224)\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mpreprocess_scan\u001b[39m\u001b[34m(path, target_size)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_scan\u001b[39m(path, target_size=(\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     volume = \u001b[43mload_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     volume = normalize_volume(volume)\n\u001b[32m     55\u001b[39m     slices = extract_12_slices(volume)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mload_scan\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_scan\u001b[39m(file_path):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m file_path.endswith(\u001b[33m'\u001b[39m\u001b[33m.nii\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m file_path.endswith(\u001b[33m'\u001b[39m\u001b[33m.nii.gz\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         img = \u001b[43mnib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img.get_fdata()\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m os.path.isdir(file_path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HenryLi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nibabel\\loadsave.py:103\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     stat_result = os.stat(filename)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo such file or no access: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stat_result.st_size <= \u001b[32m0\u001b[39m:\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ImageFileError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmpty file: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file or no access: 'root_dir = \"C:/Users/HenryLi/Downloads/Scans\"/PREFFIR-11115 (Cor CTAAdapt 0.75 B26f 75%).nii.gz'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pydicom\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "\n",
    "# ----------------------------\n",
    "# 1. SCAN LOADING & PREPROCESSING\n",
    "# ----------------------------\n",
    "def load_scan(file_path):\n",
    "    if file_path.endswith('.nii') or file_path.endswith('.nii.gz'):\n",
    "        img = nib.load(file_path)\n",
    "        return img.get_fdata()\n",
    "    elif os.path.isdir(file_path):\n",
    "        slices = [pydicom.dcmread(os.path.join(file_path, f)) for f in os.listdir(file_path) if f.endswith('.dcm')]\n",
    "        slices.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n",
    "        volume = np.stack([s.pixel_array for s in slices], axis=-1)\n",
    "        return volume\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported format\")\n",
    "\n",
    "def normalize_volume(volume):\n",
    "    volume = volume.astype(np.float32)\n",
    "    volume -= np.min(volume)\n",
    "    volume /= np.max(volume)\n",
    "    return volume\n",
    "\n",
    "def extract_12_slices(volume):\n",
    "    slices = []\n",
    "    z_len = volume.shape[2]\n",
    "    axial_indices = np.linspace(0, z_len - 1, 10, dtype=int)\n",
    "    axial_slices = [volume[:, :, idx] for idx in axial_indices]\n",
    "\n",
    "    coronal_idx = volume.shape[1] // 2\n",
    "    coronal_slice = volume[:, coronal_idx, :]\n",
    "\n",
    "    sagittal_idx = volume.shape[0] // 2\n",
    "    sagittal_slice = volume[sagittal_idx, :, :]\n",
    "\n",
    "    slices.extend(axial_slices)\n",
    "    slices.append(coronal_slice)\n",
    "    slices.append(sagittal_slice)\n",
    "\n",
    "    return slices\n",
    "\n",
    "def preprocess_scan(path, target_size=(224, 224)):\n",
    "    volume = load_scan(path)\n",
    "    volume = normalize_volume(volume)\n",
    "    slices = extract_12_slices(volume)\n",
    "    resized = [cv2.resize(s, target_size, interpolation=cv2.INTER_AREA) for s in slices]\n",
    "    return np.stack(resized, axis=0)  # (12, 224, 224)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. DATASET CLASS\n",
    "# ----------------------------\n",
    "class CTADataset(Dataset):\n",
    "    def __init__(self, labels_csv, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(labels_csv)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data.iloc[idx]\n",
    "        file_path = os.path.join(self.root_dir, entry['filename'])\n",
    "        slices = preprocess_scan(file_path)  # (12, 224, 224)\n",
    "        slices = torch.tensor(slices).unsqueeze(1).float()  # (12, 1, 224, 224)\n",
    "\n",
    "        if self.transform:\n",
    "            slices = self.transform(slices)\n",
    "\n",
    "        label = torch.tensor(entry['label']).float()\n",
    "        return slices, label\n",
    "\n",
    "# ----------------------------\n",
    "# 3. MODEL DEFINITION (2D CNN + Aggregation)\n",
    "# ----------------------------\n",
    "class SliceCNN(nn.Module):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super(SliceCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(128, feature_dim)\n",
    "\n",
    "    def forward(self, x):  # (B, 1, 224, 224)\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class CTAQuality2D(nn.Module):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super(CTAQuality2D, self).__init__()\n",
    "        self.slice_cnn = SliceCNN(feature_dim)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # (B, 12, 1, 224, 224)\n",
    "        B, S, C, H, W = x.shape\n",
    "        x = x.view(B * S, C, H, W)\n",
    "        feats = self.slice_cnn(x)  # (BÃ—12, F)\n",
    "        feats = feats.view(B, S, -1)\n",
    "        pooled = feats.mean(dim=1)\n",
    "        out = self.regressor(pooled)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. TRAINING AND EVALUATION LOOPS\n",
    "# ----------------------------\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            preds.append(outputs.cpu())\n",
    "            targets.append(labels.cpu())\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    return total_loss / len(dataloader.dataset), preds, targets\n",
    "\n",
    "# ----------------------------\n",
    "# 5. TRAINING DRIVER CODE\n",
    "# ----------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "root_dir = 'root_dir = \"C:\\\\Users\\\\HenryLi\\\\Downloads\\\\Scans\"'\n",
    "labels_csv = \"C:/Users/HenryLi/Desktop/Python Projects/Primitive Image Classifier/Labels.csv\"\n",
    "batch_size = 8\n",
    "num_epochs = 30\n",
    "\n",
    "# Load and split CSV\n",
    "df = pd.read_csv(labels_csv)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "val_df.to_csv('val.csv', index=False)\n",
    "\n",
    "train_loader = DataLoader(CTADataset('train.csv', root_dir), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(CTADataset('val.csv', root_dir), batch_size=batch_size)\n",
    "\n",
    "model = CTAQuality2D().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_preds, val_targets = evaluate(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    r2 = r2_score(val_targets.numpy(), val_preds.numpy())\n",
    "    mae = mean_absolute_error(val_targets.numpy(), val_preds.numpy())\n",
    "    print(f\"Val R^2: {r2:.3f}, MAE: {mae:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
