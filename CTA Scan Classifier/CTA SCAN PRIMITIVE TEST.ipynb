{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9f3b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=5.9466, Val Loss=5.9471\n",
      "Val R^2: -3.662, MAE: 2.161\n",
      "Epoch 2: Train Loss=5.4033, Val Loss=5.0863\n",
      "Val R^2: -2.987, MAE: 1.948\n",
      "Epoch 3: Train Loss=3.9929, Val Loss=2.8160\n",
      "Val R^2: -1.207, MAE: 1.319\n",
      "Epoch 4: Train Loss=1.8490, Val Loss=1.6317\n",
      "Val R^2: -0.279, MAE: 1.096\n",
      "Epoch 5: Train Loss=1.3640, Val Loss=1.5043\n",
      "Val R^2: -0.179, MAE: 1.052\n",
      "Epoch 6: Train Loss=1.3049, Val Loss=1.4954\n",
      "Val R^2: -0.172, MAE: 1.051\n",
      "Epoch 7: Train Loss=1.3240, Val Loss=1.4937\n",
      "Val R^2: -0.171, MAE: 1.050\n",
      "Epoch 8: Train Loss=1.2987, Val Loss=1.4796\n",
      "Val R^2: -0.160, MAE: 1.043\n",
      "Epoch 9: Train Loss=1.3011, Val Loss=1.4707\n",
      "Val R^2: -0.153, MAE: 1.040\n",
      "Epoch 10: Train Loss=1.2832, Val Loss=1.4639\n",
      "Val R^2: -0.147, MAE: 1.038\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 201\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m    200\u001b[39m     train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     val_loss, val_preds, val_targets = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     scheduler.step()\n\u001b[32m    204\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 159\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, dataloader, criterion)\u001b[39m\n\u001b[32m    157\u001b[39m preds, targets = [], []\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HenryLi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HenryLi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HenryLi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mCTADataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     81\u001b[39m scan_id = filename.split(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m1\u001b[39m].split()[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# e.g. '11001'\u001b[39;00m\n\u001b[32m     82\u001b[39m file_path = os.path.join(\u001b[38;5;28mself\u001b[39m.root_dir, scan_id, \u001b[33m'\u001b[39m\u001b[33m20\u001b[39m\u001b[33m'\u001b[39m, filename)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m slices = \u001b[43mpreprocess_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m slices = torch.tensor(slices).unsqueeze(\u001b[32m1\u001b[39m).float()\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mpreprocess_scan\u001b[39m\u001b[34m(path, target_size)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_scan\u001b[39m(path, target_size=(\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)):\n\u001b[32m     58\u001b[39m     volume = load_scan(path)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     volume = \u001b[43mnormalize_volume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvolume\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     slices = extract_12_slices(volume)\n\u001b[32m     61\u001b[39m     resized = [cv2.resize(s, target_size, interpolation=cv2.INTER_AREA) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m slices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mnormalize_volume\u001b[39m\u001b[34m(volume)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalize_volume\u001b[39m(volume):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     volume = \u001b[43mvolume\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     volume -= np.min(volume)\n\u001b[32m     36\u001b[39m     volume /= np.max(volume)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pydicom\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "\n",
    "# ----------------------------\n",
    "# 1. SCAN LOADING & PREPROCESSING\n",
    "# ----------------------------\n",
    "import pathlib\n",
    "\n",
    "def load_scan(file_path):\n",
    "    import pathlib\n",
    "    file_path = str(pathlib.Path(file_path))\n",
    "\n",
    "    if file_path.endswith('.nii') or file_path.endswith('.nii.gz'):\n",
    "        img = nib.load(file_path)\n",
    "        return img.get_fdata()\n",
    "    elif os.path.isdir(file_path):\n",
    "        slices = [pydicom.dcmread(os.path.join(file_path, f)) for f in os.listdir(file_path) if f.endswith('.dcm')]\n",
    "        slices.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n",
    "        volume = np.stack([s.pixel_array for s in slices], axis=-1)\n",
    "        return volume\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format or bad path: {file_path}\")\n",
    "\n",
    "def normalize_volume(volume):\n",
    "    volume = volume.astype(np.float32)\n",
    "    volume -= np.min(volume)\n",
    "    volume /= np.max(volume)\n",
    "    return volume\n",
    "\n",
    "def extract_12_slices(volume):\n",
    "    slices = []\n",
    "    z_len = volume.shape[2]\n",
    "    axial_indices = np.linspace(0, z_len - 1, 10, dtype=int)\n",
    "    axial_slices = [volume[:, :, idx] for idx in axial_indices]\n",
    "\n",
    "    coronal_idx = volume.shape[1] // 2\n",
    "    coronal_slice = volume[:, coronal_idx, :]\n",
    "\n",
    "    sagittal_idx = volume.shape[0] // 2\n",
    "    sagittal_slice = volume[sagittal_idx, :, :]\n",
    "\n",
    "    slices.extend(axial_slices)\n",
    "    slices.append(coronal_slice)\n",
    "    slices.append(sagittal_slice)\n",
    "\n",
    "    return slices\n",
    "\n",
    "def preprocess_scan(path, target_size=(224, 224)):\n",
    "    volume = load_scan(path)\n",
    "    volume = normalize_volume(volume)\n",
    "    slices = extract_12_slices(volume)\n",
    "    resized = [cv2.resize(s, target_size, interpolation=cv2.INTER_AREA) for s in slices]\n",
    "    return np.stack(resized, axis=0)  # (12, 224, 224)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. DATASET CLASS\n",
    "# ----------------------------\n",
    "class CTADataset(Dataset):\n",
    "    def __init__(self, labels_csv, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(labels_csv)\n",
    "        self.root_dir = r\"C:\\Users\\HenryLi\\Downloads\\Scans\"\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data.iloc[idx]\n",
    "        filename = entry['filename']\n",
    "    \n",
    "    # Assume that scan files live in: {root_dir}/{scan_id}/20/{filename}\n",
    "        scan_id = filename.split('-')[1].split()[0]  # e.g. '11001'\n",
    "        file_path = os.path.join(self.root_dir, scan_id, '20', filename)\n",
    "\n",
    "        slices = preprocess_scan(file_path)\n",
    "        slices = torch.tensor(slices).unsqueeze(1).float()\n",
    "\n",
    "        if self.transform:\n",
    "            slices = self.transform(slices)\n",
    "\n",
    "        label = torch.tensor(entry['label']).float()\n",
    "        return slices, label\n",
    "\n",
    "# ----------------------------\n",
    "# 3. MODEL DEFINITION (2D CNN + Aggregation)\n",
    "# ----------------------------\n",
    "class SliceCNN(nn.Module):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super(SliceCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(128, feature_dim)\n",
    "\n",
    "    def forward(self, x):  # (B, 1, 224, 224)\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class CTAQuality2D(nn.Module):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super(CTAQuality2D, self).__init__()\n",
    "        self.slice_cnn = SliceCNN(feature_dim)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # (B, 12, 1, 224, 224)\n",
    "        B, S, C, H, W = x.shape\n",
    "        x = x.view(B * S, C, H, W)\n",
    "        feats = self.slice_cnn(x)  # (B×12, F)\n",
    "        feats = feats.view(B, S, -1)\n",
    "        pooled = feats.mean(dim=1)\n",
    "        out = self.regressor(pooled)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. TRAINING AND EVALUATION LOOPS\n",
    "# ----------------------------\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            preds.append(outputs.cpu())\n",
    "            targets.append(labels.cpu())\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    return total_loss / len(dataloader.dataset), preds, targets\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5. TRAINING DRIVER CODE\n",
    "# ----------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "root_dir = 'root_dir = \"C:\\\\Users\\\\HenryLi\\\\Downloads\\\\Scans\"'\n",
    "labels_csv = \"C:/Users/HenryLi/Desktop/Python Projects/Primitive Image Classifier/Labels.csv\"\n",
    "batch_size = 8\n",
    "num_epochs = 30\n",
    "\n",
    "# Load and split CSV\n",
    "df = pd.read_csv(labels_csv)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "val_df.to_csv('val.csv', index=False)\n",
    "\n",
    "train_loader = DataLoader(CTADataset('train.csv', root_dir), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(CTADataset('val.csv', root_dir), batch_size=batch_size)\n",
    "\n",
    "model = CTAQuality2D().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_preds, val_targets = evaluate(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    r2 = r2_score(val_targets.numpy(), val_preds.numpy())\n",
    "    mae = mean_absolute_error(val_targets.numpy(), val_preds.numpy())\n",
    "    print(f\"Val R^2: {r2:.3f}, MAE: {mae:.3f}\")\n",
    "\n",
    "#Inspecting predictions\n",
    "print(\"Sample Predictions vs Targets:\")\n",
    "for pred, target in zip(val_preds[:10], val_targets[:10]):\n",
    "    print(f\"Pred: {pred.item():.2f}, Target: {target.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06244a9",
   "metadata": {},
   "source": [
    "## Interpretation of Results\n",
    "This is for the 1st Epoch\n",
    "| Metric       | Value     | Meaning                                                                 |\n",
    "|--------------|-----------|-------------------------------------------------------------------------|\n",
    "| **Train Loss** | 5.9466   | MSE on training data — higher means worse predictions                   |\n",
    "| **Val Loss**   | 5.9471   | MSE on validation — similar to train loss suggests underfitting         |\n",
    "| **R² (Val)**   | -3.662   | Negative R² → model is worse than predicting the mean                   |\n",
    "| **MAE**        | 2.161    | Average absolute error is ~2 points (on a 1–5 scale) — too high         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
